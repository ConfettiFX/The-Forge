//
// Copyright (c) 2016 Advanced Micro Devices, Inc. All rights reserved.
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in
// all copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
// THE SOFTWARE.

/*
 * Copyright (c) 2017-2024 The Forge Interactive Inc.
 * 
 * This file is part of The-Forge
 * (see https://github.com/ConfettiFX/The-Forge).
 * 
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 * 
 *   http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
*/

// TheForge changes
// Culling triangles that intersect the near plane
// Small primitive culling now supports MSAA
// Multi-viewport culling
// Mesh Instances: reuse the same mesh with different world transforms

#include "shader_defs.h.fsl"
#include "../../../../../../Common_3/Renderer/VisibilityBuffer/Shaders/FSL/triangle_filtering.h.fsl"

RES(ByteBuffer, indexDataBuffer, UPDATE_FREQ_NONE, t3, binding = 3);
RES(Buffer(MeshConstants), meshConstantsBuffer, UPDATE_FREQ_NONE, t4, binding = 4);
RES(Buffer(InstanceData), instanceData, UPDATE_FREQ_PER_FRAME, t5, binding = 5);

NUM_THREADS(VB_COMPUTE_THEADS, 1, 1)
void CS_MAIN( SV_GroupThreadID(uint3) threadId, SV_GroupID(uint3) groupId)
{
    INIT_MAIN;
	if (threadId.x == 0)
	{
        UNROLL_N(NUM_CULLING_VIEWPORTS)
		for (uint i = 0; i < NUM_CULLING_VIEWPORTS; ++i)
            AtomicStore(workGroupIndexCount[i], 0u);

		filterDispatchGroupData = Get(filterDispatchGroupDataBuffer)[groupId.x];
	}

    GroupMemoryBarrier();

	bool cull[NUM_CULLING_VIEWPORTS];
	uint threadOutputSlot[NUM_CULLING_VIEWPORTS];
	
	// Scope to suppressed 'uint i' variable redefinition in following loops (due to unrolling)
	{
		UNROLL_N(NUM_CULLING_VIEWPORTS)
		for (uint i = 0; i < NUM_CULLING_VIEWPORTS; ++i)
		{
			threadOutputSlot[i] = 0;
			cull[i] = true;
		}
	}

	uint batchMeshIndex = filterDispatchGroupData.meshIndex;
	uint batchGeomSet = ((filterDispatchGroupData.geometrySet_faceCount & BATCH_GEOMETRY_MASK) >> BATCH_GEOMETRY_LOW_BIT);
	uint batchInstanceDataIndex = filterDispatchGroupData.instanceDataIndex; // instanceDataIndex that comes from CPU is not encoded
	uint batchInputIndexOffset = (Get(meshConstantsBuffer)[batchMeshIndex].indexOffset + filterDispatchGroupData.indexOffset);
	uint vertexOffset = Get(meshConstantsBuffer)[batchMeshIndex].vertexOffset;
	bool twoSided = (Get(meshConstantsBuffer)[batchMeshIndex].flags & MESH_CONSTANT_FLAG_TWO_SIDED) != 0;

	if (batchInstanceDataIndex != INSTANCE_INDEX_NONE)
	{
		uint preSkinnedVtxOffset = Get(instanceData)[batchInstanceDataIndex].preSkinnedVertexOffset;
		if (preSkinnedVtxOffset != PRE_SKINNED_VERTEX_OFFSET_NONE)
		{
			// If this is an animated instance we don't use the offset in MeshConstants, that offset points to the mesh
			// in bind-pose, we need to use the pre-transformed vertexes stored at offset InstanceData::animatedAttrVertexOffset
			vertexOffset = preSkinnedVtxOffset;
		}
	}

	float4x4 mvp[NUM_CULLING_VIEWPORTS];

	for (uint i = 0; i < NUM_CULLING_VIEWPORTS; ++i)
	{
		mvp[i] = Get(transform)[i].mvp;

		if (batchInstanceDataIndex != INSTANCE_INDEX_NONE)
		{
			mvp[i] = mul(mvp[i], Get(instanceData)[batchInstanceDataIndex].modelMtx);
		}
	}

	uint indices[3] = { 0, 0, 0 };
	uint batchFaceCount =  ((filterDispatchGroupData.geometrySet_faceCount & BATCH_FACE_COUNT_MASK) >> BATCH_FACE_COUNT_LOW_BIT);
	if (threadId.x < batchFaceCount)
	{
		indices[0] = vertexOffset + LoadByte(Get(indexDataBuffer), (threadId.x * 3 + 0 + batchInputIndexOffset) << 2);
		indices[1] = vertexOffset + LoadByte(Get(indexDataBuffer), (threadId.x * 3 + 1 + batchInputIndexOffset) << 2);
		indices[2] = vertexOffset + LoadByte(Get(indexDataBuffer), (threadId.x * 3 + 2 + batchInputIndexOffset) << 2);

		float4 vert[3] =
		{
			LoadVertex(indices[0]),
			LoadVertex(indices[1]),
			LoadVertex(indices[2])
		};

        UNROLL_N(NUM_CULLING_VIEWPORTS)
		for (uint i = 0; i < NUM_CULLING_VIEWPORTS; ++i)
		{
			float4 vertices[3] =
			{
				mul(mvp[i], vert[0]),
				mul(mvp[i], vert[1]),
				mul(mvp[i], vert[2])
			};

			CullingViewPort viewport = Get(cullingViewports)[i];
			cull[i] = FilterTriangle(indices, vertices, !twoSided, viewport.windowSize, viewport.sampleCount);
			if (!cull[i])
                AtomicAdd(workGroupIndexCount[i], 3, threadOutputSlot[i]);
		}
	}

    GroupMemoryBarrier();

	uint accumBatchDrawIndex = filterDispatchGroupData.accumDrawIndex;

	if (threadId.x == 0)
	{
        UNROLL_N(NUM_CULLING_VIEWPORTS)
		for (uint i = 0; i < NUM_CULLING_VIEWPORTS; ++i)
		{
		    AtomicAdd(Get(uncompactedDrawArgsRW)[GET_UNCOMPACT_DRAW_INDEX(i,accumBatchDrawIndex)].numIndices, workGroupIndexCount[i], workGroupOutputSlot[i]);
		}
	}

    AllMemoryBarrier();

    UNROLL_N(NUM_CULLING_VIEWPORTS)
	for (uint j = 0; j < NUM_CULLING_VIEWPORTS; ++j)
	{
		if (!cull[j])
		{
            uint index = AtomicLoad(workGroupOutputSlot[j]);

			// We store indexes without vertexOffset because the offset is then stored in indirectDrawArgs and applied by the draw call
			StoreByte(Get(filteredIndicesBuffer)[j], (index + filterDispatchGroupData.outputIndexOffset + threadOutputSlot[j] + 0) << 2, indices[0] - vertexOffset);
			StoreByte(Get(filteredIndicesBuffer)[j], (index + filterDispatchGroupData.outputIndexOffset + threadOutputSlot[j] + 1) << 2, indices[1] - vertexOffset);
			StoreByte(Get(filteredIndicesBuffer)[j], (index + filterDispatchGroupData.outputIndexOffset + threadOutputSlot[j] + 2) << 2, indices[2] - vertexOffset);
		}
	}

	if (threadId.x == 0 && groupId.x == filterDispatchGroupData.meshDispatchGroupStart)
	{
        UNROLL_N(NUM_CULLING_VIEWPORTS)
		for (uint i = 0; i < NUM_CULLING_VIEWPORTS; ++i)
		{
		    uint uncompactIndex = GET_UNCOMPACT_DRAW_INDEX(i,accumBatchDrawIndex);
            Get(uncompactedDrawArgsRW)[uncompactIndex].startIndex = filterDispatchGroupData.outputIndexOffset;
			Get(uncompactedDrawArgsRW)[uncompactIndex].vertexOffset = vertexOffset;
            Get(uncompactedDrawArgsRW)[uncompactIndex].geometrySet = batchGeomSet;
            Get(uncompactedDrawArgsRW)[uncompactIndex].instanceDataIndex = 
				((batchInstanceDataIndex << INSTANCE_INDEX_LOW_BIT) & INSTANCE_INDEX_MASK) | 
				((batchMeshIndex << INSTANCE_MESH_LOW_BIT) & INSTANCE_MESH_MASK);
		}
	}
    RETURN();
}
